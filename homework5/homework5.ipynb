{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 5 - VAST Challenge 2011 - Mini Challenge 1 \n",
    "\n",
    "Authors: Suhas Keshavamurthy, Kriti Shrivastava \n",
    "\n",
    "Datasource: VAST Challenge 2011\n",
    "\n",
    "http://hcil2.cs.umd.edu/newvarepository/VAST%20Challenge%202011/taskdescription-of-all2011challenges-printfromoriginalwebisteofchallenge.pdf\n",
    "\n",
    "#### Task Description: \n",
    "Pick a VAST Challenge. This is to be the VAST Challenge for the final project.  Identify what ML algorithms you will need to run on the data. These could be one algorithm you will run several times or different algorithms. Explain your selection.  Find one or more similar example data set(s) anywhere on the web (check the data web sites the syllabus suggested) and run the algorithms. Identify which one you think will work on the VAST Challenge you selected. What visualizations do you think you need?\n",
    "\n",
    "###### Bokeh Version used: 0.12.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description:\n",
    "\n",
    "##### Mini Challenge 1  Characterization of an Epidemic Spread\n",
    "\n",
    "Vastopolis is a major metropolitan area with a population of approximately two million residents. During the last few\n",
    "days, health professionals at local hospitals have noticed a dramatic increase in reported illnesses. Observed\n",
    "symptoms are largely flu-like and include fever, chills,sweats, aches and pains, fatigue, coughing, breathing difficulty,\n",
    "nausea and vomiting, diarrhea, and enlarged lymph nodes. More recently, there have been several deaths believed\n",
    "to be associated with the current outbreak. City officials fear a possible epidemic and are mobilizing emergency\n",
    "management resources to mitigate the impact. The challenge is to provide an assessment of the\n",
    "situation.\n",
    "\n",
    "Two datasets are provided. The first one contains microblog messages collected from various devices with\n",
    "GPS capabilities. These devices include laptop computers, handheld computers, and cellular phones. The second\n",
    "one contains map information for the entire metropolitan area. The map dataset contains a satellite image with\n",
    "labeled highways, hospitals, important landmarks, and water bodies. Supplemental tables for\n",
    "population statistics and observed weather data are also made available.\n",
    "\n",
    "***MC 1.1* Origin and Epidemic Spread:** Identify approximately where the outbreak started on the map (ground zero\n",
    "location). If possible, outline the affected area. Explain how you arrived at your conclusion. (Short answer)\n",
    "\n",
    "***MC 1.2* Epidemic Spread:** Present a hypothesis on how the infection is being transmitted. For example, is the\n",
    "method of transmission person-to-person, airborne, waterborne, or something else? Identify the trends that support\n",
    "your hypothesis. Is the outbreak contained? Is it necessary for emergency management personnel to deploy\n",
    "treatment resources outside the affected area? Explain your reasoning. (Detailed answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART A\n",
    "### ML Algorithms we plan to use and why\n",
    "\n",
    "The primary source of data for this problem is the 'Twitter dataset'. The tweets provide an indication of the disease, time and location. The dataset consists of a huge number of tweets during a given time period. \n",
    "The first step is to filter the tweets related to the disease. Here we can use NLP techniques to identify the appropriate tweets by analyzing the messages. \n",
    "\n",
    "The NLP process:\n",
    "\n",
    "*Step 1:*\n",
    "Pre-processing the data. First step would be to remove stop words, remove whitespace and punctuation, lemmatization (convert to root word), and apply N-grams.\n",
    "\n",
    "*Step 2:*\n",
    "    Supervised Learning:\n",
    "    Next we will apply Word2Vec technique to convert the text to features. We can use either BOW (Continuous bag of words) or skip-gram model to do this. Before we are able to apply Word2Vec technique, we need a training set. The simplest way to do this for this particular dataset is to parse the dataset messages for few common words that we can identify is relevant for our problem. These words can be 'flu', 'sick', 'well' (and other epidemic related key words specified in the problem statement). Now we can mark these text messages as relevant and use it as the training dataset. Now we can use the Word2Vec technique to identify all relevant messages that are related to the epidemic. Between BOW and skip-gram model, we need to see which provides better results, although from an initial guess it appears that skip-gram is more relevant for our case.\n",
    "    Unsupervised Learning:\n",
    "    We can also attempt to apply unsupervised learning techniques like LDA (Latent Dirichlet allocation) to classify the messages into groups (topics). The challenge here would be to estimate number of topics (as the tweet data is going to be largely random). \n",
    "    \n",
    "*Step 3:* \n",
    "    Classification:\n",
    "    We can then use a classfication algorithm like Random Forest, SVM or Logistic Regression on the supervised learning outcome to identify relevant tweets. The accuracy needs to be calculated for each of these algorithms. If the accuracy is sufficiently high we can proceed with choosing the messages from the entire dataset.\n",
    "    It is a similar case with the unsupervised learning approach. If the estimated term frequency and overall term frequency are close to each other then we have a higher confidence that the classification generated by this model is accurate and we can use the terms with high correlation to extract relevant tweets.\n",
    "    \n",
    "Once the relevant tweets are identified we obtain the timestamp and location of the tweets corresponding to the person being sick or not. It is assumed that this location and time corresponds to the location and time of outbreak. While this is not necessarily true, over a large dataset it will help us identify the trends. \n",
    "One example of a false positive is \"Benjamin has the flu oh the suffering...\". Here the person posting the tweet may not have the flu and is referencing a third party. Her location of the tweet might also be where Benjamin is not currently situated.\n",
    "Now with this information, we can perform group-by operation on day, time, location and ID on the reduced relevant dataset.\n",
    "\n",
    "This information in itself might be sufficient to perform certain analysis.\n",
    "We can group the location information (where the disease has occured) and identify areas in Vastopolis where the epidemic is rampant. We apply clustering ML algorithms here. But since we are interested in obtaining the clusters, we need to apply algorithms like DBScan, MeanShift to identify number of clusters. We can also attempt to apply hierarchical clustering algorithms which do not require 'cluster number' as input.\n",
    "\n",
    "One additional approach can be tracking the location of all people (using the ID field). If the number of people who are sick (as identified above), and those are not sick are roughly proportionate or much less than the other, in a given area in a given time frame, then we can predict that the epidemic is not spreading from person to person.\n",
    "\n",
    "Now we can use the additional information provided in the input dataset to identify if other correlations exist for the spread of epidemic. \n",
    "\n",
    "1. We can attempt to identify if there is a correlation between wind direction and spread of the epidemic. This might indicate possible transmission over air of the disease \n",
    "\n",
    "2. It might be relevant to identify the severity of the disease by comparing the density of population with the number of diseased cases. Might indicate correlation with person-to-person transmission.\n",
    "\n",
    "3. From the map of the city, we can visually try to identify if the river (water source) might be a possible cause of the spread of the epidemic. (by observing the disease reporting location over the time-series data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART B\n",
    "### Applying approaches on sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying tweets into 2 categories: Related to Rain, Not related to Rain (when training data is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a bad boy  :  not_rain\n",
      "rain today  :  rain\n",
      "so stupid  :  not_rain\n",
      "it is raining outside  :  rain\n",
      "I love it  :  not_rain\n",
      "so good  :  not_rain\n",
      "The rainwater is everywhere!  :  not_rain\n",
      "\n",
      "Rain count: 2 \n",
      "Not Rain count: 5\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "from sys import exit\n",
    "\n",
    "## Training Data\n",
    "rain_tweets = [('this rain is craze today', 'rain'),\n",
    "               ('Nov 23 17:30 Temperature 3C no or few clouds Wind SW 6 km/h  Humidity 70% France', 'rain'),\n",
    "               ('missin climbing mountains in the rain', 'rain'),\n",
    "               ('There are days in live broadcasting Torrential rain in Paris ', 'rain'),\n",
    "               ('Heavy Rain today in!', 'rain'),\n",
    "               ('Woman in the boulangerie started complaining about the rain. I said, \"its better than terrorists\". Need to finesse my jovial patter', 'rain'),\n",
    "               ('Light to moderate rain over NCR', 'rain'),\n",
    "               ('After a cold night last night, tonight will be milder and mainly frost-free, with this band of rain. Jo', 'rain'),\n",
    "               ('But I love the rain. And it rains frequently these days~ So it makes me feel rather good', 'rain'),\n",
    "               ('With 1000 mm rain already and more rain forecasted 4 Chennai, Nov 2015 will overtake Oct 2005 and Nov 1918 to become the Wettest Month EVER!', 'rain'),\n",
    "               ('It is raining today. Wet!', 'rain'),\n",
    "               ('Lots of rain today. Raining!', 'rain'),\n",
    "               ('Why is it raining?', 'rain'),\n",
    "               ('So much rain!', 'rain'),\n",
    "               ('it always rains this time of year', 'rain'),\n",
    "               ('raining', 'rain'),\n",
    "               ('raining outside today, rained yesterday too', 'rain'),\n",
    "               ('rainy weather today! jeez', 'rain'),\n",
    "               ('Rain has finally extinguished a #wildfire in Olympic National Park that had been burning since May', 'rain'),\n",
    "               ('The rain had us indoors for Thursdays celebration', 'rain'),\n",
    "               ('Rain (hourly) 0.0 mm, Pressure: 1012 hPa, falling slowly', 'rain'),\n",
    "               ('That aspiration yours outfit make ends meet spite of the rainy weather this midsummer?: Edb', 'rain'),\n",
    "               ('Glasgow\\'s bright lights of Gordon st tonight #rain #Glasgow', 'rain'),\n",
    "               ('Why is it raining? Because it always rains this time of year', 'rain'),\n",
    "               ('The forecast for this week\\'s weather includes lots of rain!', 'rain'),\n",
    "               ('Morning Has Broken: Morning has BrokenAs I sit in my warm car in between rain squalls I am looking out', 'rain'),\n",
    "               ('Wind 2.0 mph SW. Barometer 1021.10 mb, Falling. Temperature 5.5 °C. Rain today 0.2 mm. Humidity 78%', 'rain')]\n",
    "\n",
    "not_rain_tweets = [('I do not like this car', 'not_rain'),\n",
    "              ('This view is horrible', 'not_rain'),\n",
    "              ('I feel tired this morning', 'not_rain'),\n",
    "              ('I am not looking forward to the concert', 'not_rain'),\n",
    "              ('He is my enemy', 'not_rain'),\n",
    "              ('I am a bad boy', 'not_rain'),\n",
    "              ('Tomorrow is going to be fun.', 'not_rain'),\n",
    "              ('Smiling all around.', 'not_rain'),\n",
    "              ('These are great apples today.', 'not_rain'),\n",
    "              ('How about them apples? Thomas is a happy boy.', 'not_rain'),\n",
    "              ('Thomas is very zen. He is well-mannered.', 'not_rain'),\n",
    "              ('happy and good lots of light!', 'not_rain'),\n",
    "              ('I like this new iphone very much', 'not_rain'),\n",
    "              ('This is not good', 'not_rain'),\n",
    "              ('I am bothered by this', 'not_rain'),\n",
    "              ('I am not connected with this', 'not_rain'),\n",
    "              ('Sadistic creep you ass. Die.', 'not_rain'),\n",
    "              ('All sorts of crazy and scary as hell.', 'not_rain'),\n",
    "              ('Not his emails, no.', 'not_rain'),\n",
    "              ('His father is dead. Returned obviously.', 'not_rain'),\n",
    "              ('He has a bomb.', 'not_rain'),\n",
    "              ('Too fast to be on foot. We cannot catch them.', 'not_rain'),\n",
    "              ('Feeling so stupid stoopid stupid!', 'not_rain'),\n",
    "              (':-(( :-(', 'not_rain'),\n",
    "              ('This is the worst way imaginable, all of this traffic', 'not_rain')]\n",
    "\n",
    "\n",
    "tweets = []\n",
    "for (words, sentiment) in not_rain_tweets + rain_tweets:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 2]\n",
    "    tweets.append((words_filtered, sentiment))\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "word_features = get_word_features(get_words_in_tweets(tweets))\n",
    "\n",
    "training_set = nltk.classify.apply_features(extract_features, tweets)\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "runtweets = []  # setup to import a list of tweets from a file into a python list\n",
    "## Test data\n",
    "runtweets.append('I am a bad boy')  # should be not_rain\n",
    "runtweets.append('rain today')  # should be rain\n",
    "runtweets.append('so stupid')  # should be not_rain\n",
    "runtweets.append('it is raining outside')  # should be rain\n",
    "runtweets.append('I love it')  # should be not_rain\n",
    "runtweets.append('so good')  # should be not_rain\n",
    "runtweets.append(\"The rainwater is everywhere!\") # should be rain\n",
    "not_raincount = 0\n",
    "raincount = 0\n",
    "for tweett in runtweets:\n",
    "    valued = classifier.classify(extract_features(tweett.split()))\n",
    "    print (tweett,\" : \", valued)\n",
    "    if valued == 'not_rain':\n",
    "        not_raincount = not_raincount + 1\n",
    "    if valued == 'rain':\n",
    "        raincount = raincount + 1\n",
    "print ('\\nRain count: %s \\nNot Rain count: %s' % (raincount, not_raincount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Finding similar words to create a dictionary for a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rain_down', 'rainfall', 'pelting', 'rainwater', 'rain']\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = wordnet.synsets(\"rain\")\n",
    "lemmas = list(set(chain.from_iterable([word.lemma_names() for word in synonyms])))\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorizing tweets using the category dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain Tweets:  {'rain today', 'The rainwater is everywhere!'}\n",
      "Not Rain Tweets:  {'I am a bad boy', 'so good', 'so stupid', 'it is raining outside', 'I love it'}\n"
     ]
    }
   ],
   "source": [
    "category_dict = {}\n",
    "category_dict['rain']  = lemmas\n",
    "rain_tweet = set()\n",
    "not_rain_tweet = set()\n",
    "for items in runtweets:\n",
    "        items_tweet = set(items.split())\n",
    "        if set(category_dict['rain']) & items_tweet:\n",
    "            rain_tweet.add(items)\n",
    "        else:\n",
    "            not_rain_tweet.add(items)\n",
    "print(\"Rain Tweets: \",rain_tweet)\n",
    "print(\"Not Rain Tweets: \",not_rain_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a more cohesive dictionary of relevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rain_down', 'rainfall', 'pelting', 'rainwater', 'rain', 'downfall', 'precipitation', 'deluge', 'drizzle', 'torrent', 'mizzle', 'waterspout', 'shower', 'cloudburst', 'rainstorm', 'pelter', 'soaker', 'rain_shower', 'downpour', 'monsoon', 'raindrop', 'freshwater', 'fresh_water', 'chronological_succession', 'sequence', 'successiveness', 'succession', 'chronological_sequence', 'fall', 'precipitate', 'come_down', 'spit', 'pour', 'shower_down', 'sprinkle', 'pelt', 'patter', 'stream', 'rain_buckets', 'pitter-patter', 'rain_cats_and_dogs', 'spatter']\n"
     ]
    }
   ],
   "source": [
    "### Extending dictionary to include Hypernyms, holonyms, meronyms and Hyponyms\n",
    "relevantWords = []\n",
    "relevantWords = lemmas\n",
    "relevantWordsSynsets = []\n",
    "relevantWordsSynsets = synonyms\n",
    "\n",
    "for i,j in enumerate(wordnet.synsets('rain')):\n",
    "    hypernyms = j.hypernyms()\n",
    "    hyper_lemmas = list(set(chain.from_iterable([word.lemma_names() for word in hypernyms])))\n",
    "    for word in hyper_lemmas:\n",
    "        if word not in relevantWords:\n",
    "            relevantWords.append(word)\n",
    "    hyponyms = j.hyponyms()\n",
    "    hypo_lemmas = list(set(chain.from_iterable([word.lemma_names() for word in hyponyms])))\n",
    "    for word in hypo_lemmas:\n",
    "        if word not in relevantWords:\n",
    "            relevantWords.append(word)\n",
    "    member_holonyms = j.member_holonyms()  \n",
    "    member_holonyms_lemmas = list(set(chain.from_iterable([word.lemma_names() for word in member_holonyms])))\n",
    "    for word in member_holonyms_lemmas:\n",
    "        if word not in relevantWords:\n",
    "            relevantWords.append(word)\n",
    "    part_meronyms = j.part_meronyms()\n",
    "    part_meronyms_lemmas = list(set(chain.from_iterable([word.lemma_names() for word in part_meronyms])))\n",
    "    for word in part_meronyms_lemmas:\n",
    "        if word not in relevantWords:\n",
    "            relevantWords.append(word)\n",
    "    relevantWordsSynsets.extend(hypernyms)\n",
    "    relevantWordsSynsets.extend(hyponyms)\n",
    "    relevantWordsSynsets.extend(member_holonyms)\n",
    "    relevantWordsSynsets.extend(part_meronyms)\n",
    "# print(relevantWordsSynsets)\n",
    "print(relevantWords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is a lot more cohesive than the previous one and includes word like \"drizzle\", \"monsoon\" which are also relevant to rain. Categorizing using this dictionary of words, would give better results. However, the dictionary needs to be carefully inspected for irrelevant words like \"succession\", \"sequence\" which do not match our needs and must be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain Tweets:  {'The rainwater is everywhere!', 'it is raining outside', 'rain today', 'It is pouring'}\n",
      "Not Rain Tweets:  {'I love it', 'so good', 'so stupid', 'I am a bad boy'}\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "runtweets.append(\"It is pouring\")\n",
    "# rain = Word(\"rain\")\n",
    "category_dict = {}\n",
    "category_dict['rain']  = relevantWords\n",
    "rain_tweet = set()\n",
    "not_rain_tweet = set()\n",
    "for items in runtweets:\n",
    "        items_tweet = list(items.split())\n",
    "        if set(category_dict['rain']) & set(items_tweet):\n",
    "            rain_tweet.add(items)\n",
    "        else:\n",
    "            wordFromList1 = wordnet.synsets(items_tweet[0])\n",
    "            allsyns1 = relevantWordsSynsets\n",
    "            allsyns2 = set(ss for word in items_tweet for ss in wordnet.synsets(word))\n",
    "            best = max((wordnet.wup_similarity(s1, s2) or 0, s1, s2) for s1, s2 in product(allsyns1, allsyns2))\n",
    "            if(best[0] > 0.9):\n",
    "                rain_tweet.add(items)\n",
    "            else:\n",
    "                not_rain_tweet.add(items)\n",
    "\n",
    "print(\"Rain Tweets: \",rain_tweet)\n",
    "print(\"Not Rain Tweets: \",not_rain_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after creating a more cohesive dictionary, there is still a possibility of missing out on some relevant words. One way to handle this is using semantic similarity between words. For every word in the tweet, we can find its semantic similarity with the words in our dictionary. In this way we would be able to consider words which may not be a part of our dictionary but are similar to the words in it. So if the tweet has a word which is not in our \"rain\" dictionary but is very similar to it, it will still be categorized as \"rain\".\n",
    "In the example above, the word \"pouring\" is not in the dictionary but the tweet is still categorized as \"rain\" as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART C\n",
    "### Analyzing the techniques and discussing applicability on actual dataset\n",
    "\n",
    "Approach one(using Naive Bayes classifier) works well if the training data is already provided. But in our case, we dont have the training data/ground truths. Hence we will have to look for alternatives while compromising on accuracy. The second approach creates a dictionary of the relevant words for a category. Categorization of tweets is then done using these dictionary of words. This compromises on the accuracy because there is always a chance of missing out the catch words. We have a dependency on the NLTK library for providing relevant words for the category. In this sample data, we only created a dictionary using only one word \"rain\", but in our data, we will create a dictionary using all the words mentioned in the challenge description, like, fever, chills, sweats, aches and pains etc.\n",
    "\n",
    "For the actual dataset, we can use a combination of these two techniques. We can generate the training dataset by categorizing using the dictionary technique along with manual inspection. We can then use the Nayes Bayes Classifier or other more complex nueral network to predict the class of remanining dataset. Once we have this information, we can filter the dataset and keep only the tweets which are relevant to our problem. After this is done, we can cluster these tweets based on their location and plot them over the city map provided. Plotting tweets incrementally with time, should give us an idea where the epidemic originated from and the direction in which it is spreading. We expect to face problems because of the size of actual dataset. This sample data was minuscule as compared to the original dataset and was only used to test our techniques.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART D\n",
    "### Visualizations needed\n",
    "\n",
    "The following visualizations are expected to be useful for this particular challenge\n",
    "\n",
    "Plot the location of sick reported against the background of the image of the city. Since the number of people who are sick varies over time, provide the following interactions to the user. \n",
    "1. We can tag the tweet with the location and time (displayed when user hovers over the location)\n",
    "2. We can create a play button, which plots growth of disease incidence by time against the map of Vastopolis like a time-series video.\n",
    "3. User can select date, time to plot location of sick/healthy people at a particular point of time. \n",
    "4. A line graph which shows the rate of growth/decline of the epidemic can be plotted alongside of it.\n",
    "5. A dynamic, weighted graph can also be generated as a function of time to identify relationships.\n",
    "\n",
    "Visualization can also animate wind speed and direction for a given day.\n",
    "The population density for day and night can also be indicated on the background image. (This however does not seem extremely relevant to the problem during the analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"visualization.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### References\n",
    "\n",
    "1. Simple Text Classification with Python and TextBlob: http://stevenloria.com/how-to-build-a-text-classification-system-with-python-and-textblob/\n",
    "2. Labelling tweets using supervised classification: http://mark-kay.net/2013/12/19/labelling-tweets-using-supervised-classification/\n",
    "3. Stackoverflow question on tweet classification: https://stackoverflow.com/questions/36875780/tweet-classification-into-multiple-categories-on-unsupervised-data-tweets\n",
    "4. WordNet tutorial: http://stevenloria.com/tutorial-wordnet-textblob/\n",
    "5. NLP examples in Python: https://github.com/jamesacampbell/python-examples\n",
    "6. https://blog.nycdatascience.com/student-works/identifying-fake-news-nlp/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
